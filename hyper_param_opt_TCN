#
# just trying it out..
#

# Importing necessary libraries
import matplotlib.pyplot as plt
import torch
from torch import nn
import numpy as np
from functools import partial
import os
import tempfile
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import random_split
import torchvision
import torchvision.transforms as transforms
from ray import tune
from ray import train
from ray.tune import Checkpoint, get_checkpoint
from ray.tune.schedulers import ASHAScheduler
import ray.cloudpickle as pickle
from meas_NN_classes import *
from meas_get_data import *
from meas_dataloader import *
from meas_dataset import *  
from meas_test_func_fs import *
from ray.tune.search.optuna import OptunaSearch
from ray import tune
import hyperopt
from ray.tune.search import ConcurrencyLimiter


device = "cuda:0" if torch.cuda.is_available() else "cpu"
#device = "cpu"
print(device)

def train_cifar(config, data_dir=None):
    
    num_channels = [config["n_hidden"]] * config["levels"]
    model = OR_TCN(input_size=12, output_size=6 , num_channels=num_channels,
                        kernel_size=config["kernel_size"], dropout=config["dropout"], windowsize=config["ws"]).to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr = config["lr"])


    checkpoint = get_checkpoint()
    if checkpoint:
        with checkpoint.as_directory() as checkpoint_dir:
            data_path = Path(checkpoint_dir) / "data.pkl"
            with open(data_path, "rb") as fp:
                checkpoint_state = pickle.load(fp)
            start_epoch = checkpoint_state["epoch"]
            model.load_state_dict(checkpoint_state["net_state_dict"])
            optimizer.load_state_dict(checkpoint_state["optimizer_state_dict"])
    else:
        start_epoch = 0

    #traindataloader, test_data_loader = load_data(config)

    ###TRYING OUT THE DATALOADER
    params = {"model_flag": "OR_TCN", 
              "ws": int(config["ws"]),
              "b_s": int(config["b_s"]),
              "cut_off_timesteps": 0,
              "percentage_of_data": 0.8
              }
    train_data, test_data = get_data_robot()
    #Split data into train and test sets
    np.random.seed(1234)
    num_of_inits_train = int(len(train_data)*params["percentage_of_data"])
    train_inits = np.random.choice(np.arange(len(train_data)),num_of_inits_train,replace=False)
    test_inits = np.array([x for x in range(len(train_data)) if x not in train_inits])
    np.random.shuffle(train_inits)
    np.random.shuffle(test_inits)
    #train_data = input_data[train_inits,:input_data.size(dim=1)-params["cut_off_timesteps"],:]
    #cut off timesteps at the start
    train_data = train_data[train_inits,params["cut_off_timesteps"]:,:]
    #test_data = train_data[test_inits,:,:]

    # dataloader for batching during training
    if "OR" in params["model_flag"]:
        train_set = custom_simple_dataset(train_data, window_size=int(params["ws"]))
        traindataloader = DataLoader(train_set, batch_size=int(params["b_s"]), pin_memory=True)
    
    ###################
    
    for epoch in range(start_epoch, 100):

        loss_fn = nn.MSELoss()
        model.train()
        total_loss = []
        #device = next(model.parameters()).device

        for k, (x,y) in enumerate(traindataloader):  
            
            x = x.to(device)
            y = y.to(device)

            if model.get_flag() in ["OR_LSTM", "OR_RNN", "OR_GRU"]:
                output, _ = model(x)
            if model.get_flag() == "OR_MLP":
                output = model(x)
            if model.get_flag() == "OR_TCN":
                x = x.transpose(1,2)
                y = y.transpose(1,2)
                output = model(x)

            optimizer.zero_grad(set_to_none=True)

            loss = loss_fn(output, y) # + lambda * loss_physics

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            total_loss.append(loss.item())
            loss_mean_all_traj = np.mean(total_loss)


        checkpoint_data = {
            "epoch": epoch,
            "net_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
        }
        with tempfile.TemporaryDirectory() as checkpoint_dir:
            data_path = Path(checkpoint_dir) / "data.pkl"
            with open(data_path, "wb") as fp:
                pickle.dump(checkpoint_data, fp)

            checkpoint = Checkpoint.from_directory(checkpoint_dir)
            tune.report(
                {"loss": loss_mean_all_traj},
                checkpoint=checkpoint,
            ) # {"loss": val_loss / val_steps, "accuracy": correct / total},


def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2, model_key=""):
   

    # data_dir = os.path.abspath("./data")
    #custom_temp_dir = "C:/ray_temp"
    if os.name == "nt":
     custom_temp_dir=r"C:/ray_temp"
    else:
     custom_temp_dir=r"/home/rdpusr/ray_temp"

    # Ensure the directory exists
    if not os.path.exists(custom_temp_dir):
        os.makedirs(custom_temp_dir)
    data_dir = custom_temp_dir

    config = {
        "n_hidden": tune.choice([4,5]),
        "levels": tune.choice([4,5]),
        "lr": tune.loguniform(1e-4, 1e-2),
        "b_s": tune.choice([8, 16, 20, 26, 32, 40]), #difficult because of dataloader and dataset...
        "kernel_size" : tune.choice([5, 6, 7]),
        "dropout" : 0,
        "ws" : tune.choice([8,16,24,32, 48])
    }

    #load_data(config, data_dir)

    scheduler = ASHAScheduler(
        metric="loss",
        mode="min",
        max_t=max_num_epochs,
        grace_period=2,
        reduction_factor=2,
    )
    #algo = OptunaSearch(metric="loss", mode="min")
    #algo = ConcurrencyLimiter(algo, max_concurrent=4)
    #search_alg=algo,

    result = tune.run(
        partial(train_cifar, data_dir=data_dir),
        resources_per_trial={"cpu": 16, "gpu": gpus_per_trial},
        config=config,
        num_samples=num_samples,
        
        scheduler=scheduler,
        log_to_file=True,
        storage_path=custom_temp_dir,
    )

    best_trial = result.get_best_trial("loss", "min", "last")
    print(f"Best trial config: {best_trial.config}")
    print(f"Best trial final validation loss: {best_trial.last_result['loss']}")

    #Test the best model
    device = "cpu"
    inp_size, output_size = 12, 6


    if model_key == "OR_TCN":
        num_channels = [best_trial.config["n_hidden"]] * best_trial.config["levels"]
        best_trained_model = OR_TCN(input_size=inp_size, output_size=output_size , num_channels=num_channels,
                        kernel_size=best_trial.config["kernel_size"], dropout=best_trial.config["dropout"], windowsize=best_trial.config["ws"]).to(device)

    with open("hyper_opt_results.txt", "a") as file:
        file.write(best_trained_model.get_flag() + "\n" + "loss" + str(best_trial.last_result['loss']) + str(best_trial.config) + "\n" + "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-" "\n")
    

    if torch.cuda.is_available():
        device = "cuda:0"
        if gpus_per_trial > 1:
            best_trained_model = nn.DataParallel(best_trained_model)
    best_trained_model.to(device)

    best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric="loss", mode="max")
    with best_checkpoint.as_directory() as checkpoint_dir:
        data_path = Path(checkpoint_dir) / "data.pkl"
        with open(data_path, "rb") as fp:
            best_checkpoint_data = pickle.load(fp)


        train_data, test_data = get_data_robot()
        params = {"model_flag": "OR_TCN", 
        "window_size": best_trial.config["ws"],
        "batch_size": best_trial.config["b_s"],
        "cut_off_timesteps": 0,
        "percentage_of_data": 0.8
        }
        train_loader, test_data = get_dataloader(train_data, params)
        best_trained_model.load_state_dict(best_checkpoint_data["net_state_dict"])
       
        test_acc = test(data=test_data, model=best_trained_model, window_size=best_trial.config["ws"])
        print(best_trained_model.get_flag(), "Best trial test set loss: {}".format(test_acc))


if __name__ == "__main__":
    # You can change the number of GPUs per trial here:
    model_keys =["OR_TCN"] 
    for model_key in model_keys:
        main(num_samples=5, max_num_epochs=5, gpus_per_trial=1, model_key=model_key)

# OR_LSTM : Best trial config: {'h_size': 46, 'lr': 0.0012515516643351928, 'b_s': 8, 'l_num': 2, 'ws': 48}
# OR_MLP : 