#
# just trying it out..
#

# Importing necessary libraries
import matplotlib.pyplot as plt
import torch
from torch import nn
import numpy as np
from functools import partial
import os
import tempfile
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import random_split
import torchvision
import torchvision.transforms as transforms
from ray import tune
from ray import train
from ray.train import Checkpoint, get_checkpoint
from ray.tune.schedulers import ASHAScheduler
import ray.cloudpickle as pickle
from meas_NN_classes import *
from meas_get_data import *
from meas_dataloader import *
from meas_dataset import *  
from meas_test_func_fs import *

device = "cuda:0" if torch.cuda.is_available() else "cpu"


# def load_data(config, data_dir=None):

#     params = {"model_flag": "OR_LSTM", 
#               "ws": 10,
#               "b_s": int(config["b_s"]),
#               "cut_off_timesteps": 0,
#               "percentage_of_data": 0.8
#               }
#     train_data, test_data = get_data_robot()
#     train_data_loader, test_data_unused = get_dataloader(train_data, params)
#     test_data_loader, test_data_unused = get_dataloader(test_data, params)

#     return train_data_loader, test_data_loader, params


def train_cifar(config, data_dir=None):

    model = OR_LSTM(input_size=12, hidden_size=config["h_size"], out_size=6, layers=config["l_num"], window_size=config["ws"], flag="OR_LSTM").to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr = config["lr"])


    checkpoint = get_checkpoint()
    if checkpoint:
        with checkpoint.as_directory() as checkpoint_dir:
            data_path = Path(checkpoint_dir) / "data.pkl"
            with open(data_path, "rb") as fp:
                checkpoint_state = pickle.load(fp)
            start_epoch = checkpoint_state["epoch"]
            model.load_state_dict(checkpoint_state["net_state_dict"])
            optimizer.load_state_dict(checkpoint_state["optimizer_state_dict"])
    else:
        start_epoch = 0

    #traindataloader, test_data_loader = load_data(config)

    ###TRYING OUT THE DATALOADER
    params = {"model_flag": "OR_LSTM", 
              "ws": 10,
              "b_s": int(config["b_s"]),
              "cut_off_timesteps": 0,
              "percentage_of_data": 0.8
              }
    train_data, test_data = get_data_robot()
    #Split data into train and test sets
    np.random.seed(1234)
    num_of_inits_train = int(len(train_data)*params["percentage_of_data"])
    train_inits = np.random.choice(np.arange(len(train_data)),num_of_inits_train,replace=False)
    test_inits = np.array([x for x in range(len(train_data)) if x not in train_inits])
    np.random.shuffle(train_inits)
    np.random.shuffle(test_inits)
    #train_data = input_data[train_inits,:input_data.size(dim=1)-params["cut_off_timesteps"],:]
    #cut off timesteps at the start
    train_data = train_data[train_inits,params["cut_off_timesteps"]:,:]
    test_data = train_data[test_inits,:,:]

    # dataloader for batching during training
    if "OR" in params["model_flag"]:
        train_set = custom_simple_dataset(train_data, window_size=params["w_s"])
        traindataloader = DataLoader(train_set, batch_size=int(params["b_s"]), pin_memory=True)
    ###################
    
    for epoch in range(start_epoch, 10):

        loss_fn = nn.MSELoss()
        model.train()
        total_loss = []
        #device = next(model.parameters()).device

        for k, (x,y) in enumerate(traindataloader):  
            
            x = x.to(device)
            y = y.to(device)

            if model.get_flag() in ["OR_LSTM", "OR_RNN", "OR_GRU"]:
                output, _ = model(x)
            if model.get_flag() == "OR_MLP":
                output = model(x)
            if model.get_flag() == "OR_TCN":
                x = x.transpose(1,2)
                y = y.transpose(1,2)
                output = model(x)

            optimizer.zero_grad(set_to_none=True)

            loss = loss_fn(output, y) # + lambda * loss_physics

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            total_loss.append(loss.item())
            loss_mean_all_traj = np.mean(total_loss)


        checkpoint_data = {
            "epoch": epoch,
            "net_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
        }
        with tempfile.TemporaryDirectory() as checkpoint_dir:
            data_path = Path(checkpoint_dir) / "data.pkl"
            with open(data_path, "wb") as fp:
                pickle.dump(checkpoint_data, fp)

            checkpoint = Checkpoint.from_directory(checkpoint_dir)
            train.report(
                {"loss": loss_mean_all_traj},
                checkpoint=checkpoint,
            ) # {"loss": val_loss / val_steps, "accuracy": correct / total},


def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):
   
    #data_dir = os.path.abspath("./data")
    custom_temp_dir = "C:/ray_temp"

    # Ensure the directory exists
    if not os.path.exists(custom_temp_dir):
        os.makedirs(custom_temp_dir)
    data_dir = custom_temp_dir

    config = {
        "h_size": tune.choice([2,4,8,16]),
        "lr": tune.loguniform(1e-4, 1e-2),
        "b_s": tune.choice([6,16,32]), #difficult because of dataloader and dataset...
    }

    #load_data(config, data_dir)

    scheduler = ASHAScheduler(
        metric="loss",
        mode="min",
        max_t=max_num_epochs,
        grace_period=1,
        reduction_factor=2,
    )
    result = tune.run(
        partial(train_cifar, data_dir=data_dir),
        resources_per_trial={"cpu": 2, "gpu": gpus_per_trial},
        config=config,
        num_samples=num_samples,
        scheduler=scheduler,
        log_to_file=True,
        storage_path=custom_temp_dir,
    )

    best_trial = result.get_best_trial("loss", "min", "last")
    print(f"Best trial config: {best_trial.config}")
    print(f"Best trial final validation loss: {best_trial.last_result['loss']}")
    print(f"Best trial final validation accuracy: {best_trial.last_result['accuracy']}")

    model = OR_LSTM(input_size=12, hidden_size=best_trial.config["h_size"], out_size=6,
                     layers=best_trial.config["l_num"], window_size=best_trial.config["ws"], flag="OR_LSTM").to(device)
    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda:0"
        if gpus_per_trial > 1:
            best_trained_model = nn.DataParallel(model)
    best_trained_model.to(device)

    best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric="accuracy", mode="max")
    with best_checkpoint.as_directory() as checkpoint_dir:
        data_path = Path(checkpoint_dir) / "data.pkl"
        with open(data_path, "rb") as fp:
            best_checkpoint_data = pickle.load(fp)

        train_data, test_data = get_data_robot()
        train_loader, test_data = get_dataloader(train_data, best_trial.config)
        best_trained_model.load_state_dict(best_checkpoint_data["net_state_dict"])
       
        test_acc = test(data=test_data, model=best_trained_model, device=device, window_size=best_trial.config["ws"])
        print("Best trial test set accuracy: {}".format(test_acc))


if __name__ == "__main__":
    # You can change the number of GPUs per trial here:
    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)